{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9919494,"sourceType":"datasetVersion","datasetId":6096226}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:05:52.788362Z","iopub.execute_input":"2024-11-20T08:05:52.788895Z","iopub.status.idle":"2024-11-20T08:06:03.574674Z","shell.execute_reply.started":"2024-11-20T08:05:52.788835Z","shell.execute_reply":"2024-11-20T08:06:03.573803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset, DatasetDict\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport evaluate\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:06:03.576245Z","iopub.execute_input":"2024-11-20T08:06:03.576555Z","iopub.status.idle":"2024-11-20T08:06:26.165973Z","shell.execute_reply.started":"2024-11-20T08:06:03.576523Z","shell.execute_reply":"2024-11-20T08:06:26.16504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndataset_dir = \"/kaggle/input/tokenized-dataset/\"\nprint(\"Contents of the dataset directory:\")\nprint(os.listdir(dataset_dir))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:06:31.675045Z","iopub.execute_input":"2024-11-20T08:06:31.675713Z","iopub.status.idle":"2024-11-20T08:06:31.685894Z","shell.execute_reply.started":"2024-11-20T08:06:31.675677Z","shell.execute_reply":"2024-11-20T08:06:31.684952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/tokenized-dataset/Tokenized_Data.txt\"\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    lines = file.readlines()\n\nprint(\"First 5 lines of the tokenized dataset:\")\nfor line in lines[:5]:\n    print(line.strip())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:06:33.982559Z","iopub.execute_input":"2024-11-20T08:06:33.982885Z","iopub.status.idle":"2024-11-20T08:06:50.974666Z","shell.execute_reply.started":"2024-11-20T08:06:33.982859Z","shell.execute_reply":"2024-11-20T08:06:50.973769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Hugging Face model name and access token\nmodel_name = \"meta-llama/Llama-3.2-1B\"\naccess_token = \"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\"\n \ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=access_token)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total Parameters: {total_params:,}\")\nprint(f\"Trainable Parameters: {trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:06:54.259271Z","iopub.execute_input":"2024-11-20T08:06:54.259972Z","iopub.status.idle":"2024-11-20T08:07:59.151503Z","shell.execute_reply.started":"2024-11-20T08:06:54.259937Z","shell.execute_reply":"2024-11-20T08:07:59.150582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SST","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AutoModelForQuestionAnswering,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n)\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:08:01.436261Z","iopub.execute_input":"2024-11-20T08:08:01.436732Z","iopub.status.idle":"2024-11-20T08:08:01.44123Z","shell.execute_reply.started":"2024-11-20T08:08:01.436699Z","shell.execute_reply":"2024-11-20T08:08:01.440358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Loading SST-2 dataset\nsst2_dataset = load_dataset(\"glue\", \"sst2\")\n\nSEED = 1\nsplit_sst2 = sst2_dataset[\"train\"].train_test_split(test_size=0.2, seed=SEED)\ntrain_sst2 = split_sst2[\"train\"]\ntest_sst2 = split_sst2[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:08:04.725301Z","iopub.execute_input":"2024-11-20T08:08:04.725675Z","iopub.status.idle":"2024-11-20T08:08:07.8564Z","shell.execute_reply.started":"2024-11-20T08:08:04.725644Z","shell.execute_reply":"2024-11-20T08:08:07.855543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_sst2(examples):\n    return tokenizer_sst2(\n        examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128\n    )\n# Loading tokenizer for classification\ntokenizer_sst2 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ntrain_sst2 = train_sst2.map(preprocess_sst2, batched=True)\ntest_sst2 = test_sst2.map(preprocess_sst2, batched=True)\n\ntrain_sst2.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_sst2.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:08:10.84038Z","iopub.execute_input":"2024-11-20T08:08:10.841406Z","iopub.status.idle":"2024-11-20T08:08:20.656507Z","shell.execute_reply.started":"2024-11-20T08:08:10.841353Z","shell.execute_reply":"2024-11-20T08:08:20.655548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\nimport os\nimport numpy as np\nfrom transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n\nmodel_sst2 = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-3.2-1B\", num_labels=2)\n\n# Defining training arguments\ntraining_args_sst2 = TrainingArguments(\n    output_dir=\"./sst2_model\",  \n    eval_strategy=\"epoch\", \n    save_strategy=\"epoch\",  \n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    seed=SEED,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    save_total_limit=3,  \n    overwrite_output_dir=True,  \n)\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\ncheckpoint = None\nif os.path.isdir(training_args_sst2.output_dir):\n    checkpoint = os.path.join(training_args_sst2.output_dir, \"checkpoint-last\")\n\ntrainer_sst2 = Trainer(\n    model=model_sst2,\n    args=training_args_sst2,\n    train_dataset=train_sst2,\n    eval_dataset=test_sst2,\n    tokenizer=tokenizer_sst2,\n    compute_metrics=compute_metrics,\n)\n\ntrainer_sst2.train(resume_from_checkpoint=checkpoint)\ntrainer_sst2.save_model(training_args_sst2.output_dir)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:08:23.348177Z","iopub.execute_input":"2024-11-20T08:08:23.348603Z","iopub.status.idle":"2024-11-20T08:51:34.243172Z","shell.execute_reply.started":"2024-11-20T08:08:23.34857Z","shell.execute_reply":"2024-11-20T08:51:34.24246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FIne-tuned parameters\ntotal_params = sum(p.numel() for p in model_sst2.parameters())  \ntrainable_params = sum(p.numel() for p in model_sst2.parameters() if p.requires_grad)  \n\nprint(f\"Total Parameters in Fine-Tuned Model: {total_params:,}\")\nprint(f\"Trainable Parameters in Fine-Tuned Model: {trainable_params:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T08:59:40.822321Z","iopub.execute_input":"2024-11-20T08:59:40.822711Z","iopub.status.idle":"2024-11-20T08:59:40.831431Z","shell.execute_reply.started":"2024-11-20T08:59:40.822679Z","shell.execute_reply":"2024-11-20T08:59:40.830574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef get_gpu_with_most_memory():\n    num_gpus = torch.cuda.device_count()\n    if num_gpus == 0:\n        return \"cpu\" \n    \n    max_memory = 0\n    selected_device = 0\n    for i in range(num_gpus):\n        memory_allocated = torch.cuda.memory_allocated(i)\n        memory_reserved = torch.cuda.memory_reserved(i)\n        total_memory = torch.cuda.get_device_properties(i).total_memory\n        \n        free_memory = total_memory - memory_allocated - memory_reserved\n        \n        if free_memory > max_memory:\n            max_memory = free_memory\n            selected_device = i\n    \n    return f\"cuda:{selected_device}\"\n\n\ndevice = torch.device(get_gpu_with_most_memory())\n\ntokenizer_sst2 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Defining the zero-shot model (Llama-3.2-1B)\ndef get_zero_shot_model():\n    zero_shot_classifier = pipeline(\n        \"zero-shot-classification\",\n        model=\"meta-llama/Llama-3.2-1B\",\n        tokenizer=\"meta-llama/Llama-3.2-1B\",\n        use_auth_token=\"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\",\n        device=device.index if device.type == 'cuda' else -1  \n    )\n    return zero_shot_classifier\n\ndef int_to_label(int_label):\n    label_map = {0: \"negative\", 1: \"positive\"}\n    return label_map.get(int_label, \"unknown\")\n\n# Function to calculate metrics for zero-shot model\ndef calculate_metrics_zero_shot(zero_shot_classifier, test_dataset, labels):\n    all_preds = []\n    all_labels = []\n    \n    for batch in test_dataset:\n        text = tokenizer_sst2.decode(batch['input_ids'], skip_special_tokens=True)\n        \n        true_label = int_to_label(batch['label'].item())  \n\n        result = zero_shot_classifier(text, candidate_labels=labels)\n        predicted_label = result['labels'][0]  \n\n        all_preds.append(predicted_label)\n        all_labels.append(true_label)\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='binary', pos_label='positive')\n    recall = recall_score(all_labels, all_preds, average='binary', pos_label='positive')\n    f1 = f1_score(all_labels, all_preds, average='binary', pos_label='positive')\n    \n    return accuracy, precision, recall, f1\n\nlabels = [\"positive\", \"negative\"]  \n\n\nzero_shot_classifier = get_zero_shot_model()\naccuracy_zero_shot, precision_zero_shot, recall_zero_shot, f1_zero_shot = calculate_metrics_zero_shot(zero_shot_classifier, test_sst2, labels)\n\nprint(f\"Zero-shot Model Metrics:\")\nprint(f\"Accuracy: {accuracy_zero_shot}\")\nprint(f\"Precision: {precision_zero_shot}\")\nprint(f\"Recall: {recall_zero_shot}\")\nprint(f\"F1: {f1_zero_shot}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T09:05:45.889985Z","iopub.execute_input":"2024-11-20T09:05:45.890327Z","iopub.status.idle":"2024-11-20T09:24:09.074049Z","shell.execute_reply.started":"2024-11-20T09:05:45.890295Z","shell.execute_reply":"2024-11-20T09:24:09.073169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport numpy as np\nimport evaluate\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef get_gpu_with_most_memory():\n    num_gpus = torch.cuda.device_count()\n    if num_gpus == 0:\n        return \"cpu\"  \n\n    max_free_memory = 0\n    selected_device = 0\n    for i in range(num_gpus):\n        free_memory = torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_reserved(i)\n        if free_memory > max_free_memory:\n            max_free_memory = free_memory\n            selected_device = i\n\n    return f\"cuda:{selected_device}\"\n\ndevice = torch.device(get_gpu_with_most_memory())\n\n# Loading fine-tuned model\nmodel_fine_tuned = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/working/sst2_model\",\n    num_labels=2\n)\nmodel_fine_tuned.to(device) \n\n# Defining training arguments for evaluation\ntraining_args_fine_tuned = TrainingArguments(\n    output_dir=\"./fine_tuned_model\",\n    per_device_eval_batch_size=16,\n    no_cuda=device.type == \"cpu\", \n    evaluation_strategy=\"epoch\",  \n    save_strategy=\"epoch\", \n    load_best_model_at_end=True,  \n)\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n\n    accuracy = metric.compute(predictions=predictions, references=labels)['accuracy']\n    precision = precision_score(labels, predictions, average='binary')\n    recall = recall_score(labels, predictions, average='binary')\n    f1 = f1_score(labels, predictions, average='binary')\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n    }\n\n# Creating Trainer for fine-tuned model evaluation\ntrainer_fine_tuned = Trainer(\n    model=model_fine_tuned,\n    args=training_args_fine_tuned,\n    eval_dataset=test_sst2,  \n    compute_metrics=compute_metrics,\n)\n\neval_results_fine_tuned = trainer_fine_tuned.evaluate()\n\nprint(f\"Fine-tuned Model Metrics:\")\nprint(f\"Accuracy: {eval_results_fine_tuned['eval_accuracy']}\")\nprint(f\"Precision: {eval_results_fine_tuned['eval_precision']}\")\nprint(f\"Recall: {eval_results_fine_tuned['eval_recall']}\")\nprint(f\"F1: {eval_results_fine_tuned['eval_f1']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T09:45:42.898745Z","iopub.execute_input":"2024-11-20T09:45:42.899425Z","iopub.status.idle":"2024-11-20T09:46:54.027184Z","shell.execute_reply.started":"2024-11-20T09:45:42.899395Z","shell.execute_reply":"2024-11-20T09:46:54.026348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Zero-shot Model Metrics:\")\nprint(f\"Accuracy: {accuracy_zero_shot}\")\nprint(f\"Precision: {precision_zero_shot}\")\nprint(f\"Recall: {recall_zero_shot}\")\nprint(f\"F1: {f1_zero_shot}\")\n\nprint(\"\\nFine-tuned Model Metrics:\")\nprint(f\"Accuracy: {eval_results_fine_tuned['eval_accuracy']}\")\nprint(f\"Precision: {eval_results_fine_tuned['eval_precision']}\")\nprint(f\"Recall: {eval_results_fine_tuned['eval_recall']}\")\nprint(f\"F1: {eval_results_fine_tuned['eval_f1']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T09:47:00.851376Z","iopub.execute_input":"2024-11-20T09:47:00.852317Z","iopub.status.idle":"2024-11-20T09:47:00.862023Z","shell.execute_reply.started":"2024-11-20T09:47:00.852267Z","shell.execute_reply":"2024-11-20T09:47:00.861039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
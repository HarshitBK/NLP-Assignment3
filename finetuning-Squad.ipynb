{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-22T21:56:17.233421Z","iopub.status.busy":"2024-11-22T21:56:17.233048Z","iopub.status.idle":"2024-11-22T21:56:18.321956Z","shell.execute_reply":"2024-11-22T21:56:18.321052Z","shell.execute_reply.started":"2024-11-22T21:56:17.233390Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:14:36.299621Z","iopub.status.busy":"2024-11-22T23:14:36.299322Z","iopub.status.idle":"2024-11-22T23:14:45.961289Z","shell.execute_reply":"2024-11-22T23:14:45.960399Z","shell.execute_reply.started":"2024-11-22T23:14:36.299593Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n"]}],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:35:39.094832Z","iopub.status.busy":"2024-11-22T23:35:39.093976Z","iopub.status.idle":"2024-11-22T23:35:58.809246Z","shell.execute_reply":"2024-11-22T23:35:58.808357Z","shell.execute_reply.started":"2024-11-22T23:35:39.094791Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from datasets import load_dataset, DatasetDict\n","from datasets import Dataset, DatasetDict\n","from sklearn.model_selection import train_test_split\n","import torch\n","import evaluate\n","import numpy as np\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:36:04.467400Z","iopub.status.busy":"2024-11-22T23:36:04.466464Z","iopub.status.idle":"2024-11-22T23:37:20.869609Z","shell.execute_reply":"2024-11-22T23:37:20.868821Z","shell.execute_reply.started":"2024-11-22T23:36:04.467341Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81dbb12e496d4f16a87f3ddaef421e72","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6d141b6bd754af19a1ccc9fe5fd8e77","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"141410dd80c846ab832611a8f27b7c3b","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf8c2eb9ad8e449ba4dc2a7788ff38dd","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7810b8d6288a4b6bb2da5154ed0b3c63","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Total Parameters: 1,235,818,498\n","Trainable Parameters: 4,098\n"]}],"source":["from transformers import AutoModelForQuestionAnswering\n","import torch\n","\n","\n","model_name = \"meta-llama/Llama-3.2-1B\"\n","# Access_token goes here\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(\n","    model_name,\n","    use_auth_token=access_token\n",")\n","\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","\n","for param in model.qa_outputs.parameters():\n","    param.requires_grad = True\n","    \n","\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","print(f\"Total Parameters: {total_params:,}\")\n","print(f\"Trainable Parameters: {trainable_params:,}\")\n","\n","#Took the suggestion of training only last layer by freezing other parameters to overcome \"Out of Memory issue\" from fellow students.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:37:30.665650Z","iopub.status.busy":"2024-11-22T23:37:30.664997Z","iopub.status.idle":"2024-11-22T23:37:30.669464Z","shell.execute_reply":"2024-11-22T23:37:30.668565Z","shell.execute_reply.started":"2024-11-22T23:37:30.665616Z"},"trusted":true},"outputs":[],"source":["# model = model.full()\n","# model.gradient_checkpointing_enable()\n","model.config.use_cache = False\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:37:33.012481Z","iopub.status.busy":"2024-11-22T23:37:33.011763Z","iopub.status.idle":"2024-11-22T23:37:33.016884Z","shell.execute_reply":"2024-11-22T23:37:33.015997Z","shell.execute_reply.started":"2024-11-22T23:37:33.012446Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","0\n"]}],"source":["print(torch.cuda.memory_allocated()) \n","print(torch.cuda.memory_reserved()) "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:37:36.152029Z","iopub.status.busy":"2024-11-22T23:37:36.151362Z","iopub.status.idle":"2024-11-22T23:37:36.156541Z","shell.execute_reply":"2024-11-22T23:37:36.155613Z","shell.execute_reply.started":"2024-11-22T23:37:36.151995Z"},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    AutoModelForQuestionAnswering,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n",")\n","import evaluate"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:38:41.625692Z","iopub.status.busy":"2024-11-22T23:38:41.625027Z","iopub.status.idle":"2024-11-22T23:38:44.310591Z","shell.execute_reply":"2024-11-22T23:38:44.309809Z","shell.execute_reply.started":"2024-11-22T23:38:41.625660Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"474f62f2555247c3acc16761385a5f99","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23f9c1eb6b694ca480777c47b60d46d5","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93cacbda4e7a49b2aba524134fcb55d4","version_major":2,"version_minor":0},"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24f62c93c9124f91b9af8326d7f64063","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e704218fb822421a8c2ec57bb057d410","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["['id', 'title', 'context', 'question', 'answers']\n","Imperial's Royal Charter\n"]}],"source":["# from datasets import load_dataset\n","\n","squad_v2 = load_dataset(\"rajpurkar/squad_v2\")\n","\n","# print(squad_v2)\n","\n","SEED = 1\n","split_squad_v2 = squad_v2[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n","train_squad_v2 = split_squad_v2[\"train\"]\n","test_squad_v2 = split_squad_v2[\"test\"]\n","\n","\n","import copy\n","\n","testing_copy_2 = copy.deepcopy(split_squad_v2[\"test\"])\n","testing_copy_2 = testing_copy_2.select(range(30))\n","# print(testing_copy_2)\n","print(train_squad_v2.column_names)\n","\n","print(testing_copy_2[\"answers\"][0]['text'][0])\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:38:49.692419Z","iopub.status.busy":"2024-11-22T23:38:49.691742Z","iopub.status.idle":"2024-11-22T23:38:49.699629Z","shell.execute_reply":"2024-11-22T23:38:49.698676Z","shell.execute_reply.started":"2024-11-22T23:38:49.692380Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","from datasets import load_dataset\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","\n","# def preprocess_batch_llama(batch):\n","#     \n","#     inputs = [\n","#         f\"Context: {context}\\nQuestion: {question}\\nAnswer: {answer.get('text')}\"\n","#         for context, question, answer in zip(batch['context'], batch['question'], batch['answers'])\n","#     ]\n","    \n","#     \n","#     tokenized = tokenizer(\n","#         inputs,\n","#         max_length=512,\n","#         padding=\"max_length\",\n","#         truncation=True,\n","#         return_tensors=\"pt\"\n","#     )\n","    \n","#     \n","#     labels = tokenized[\"input_ids\"].clone()\n","#     labels[labels == tokenizer.pad_token_id] = -100\n","#     tokenized[\"labels\"] = labels\n","    \n","#     return tokenized\n","\n","def preprocess_batch_llama(examples):\n","    \n","    inputs = tokenizer(\n","        examples[\"context\"],\n","        examples[\"question\"],\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=512,  \n","        return_tensors=\"pt\"\n","    )\n","\n","    \n","    start_positions = []\n","    end_positions = []\n","\n","    for i, answer in enumerate(examples[\"answers\"]):\n","        if len(answer['text']) == 0:  \n","            start_positions.append(-1)\n","            end_positions.append(-1)\n","        else:  \n","            if len(answer[\"answer_start\"]) > 0:  \n","                start = answer[\"answer_start\"][0]\n","                end = start + len(answer[\"text\"][0]) \n","                start_positions.append(start)\n","                end_positions.append(end)\n","            else:\n","              \n","                start_positions.append(-1)\n","                end_positions.append(-1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:38:52.149766Z","iopub.status.busy":"2024-11-22T23:38:52.149363Z","iopub.status.idle":"2024-11-22T23:38:52.383335Z","shell.execute_reply":"2024-11-22T23:38:52.382515Z","shell.execute_reply.started":"2024-11-22T23:38:52.149728Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4851c14d33d6441d84c03a27f0e02703","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/30 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["3.999961632903622\n"]}],"source":["\n","# train_squad_v2 = train_squad_v2.map(\n","#     preprocess_batch_llama, \n","#     batched=True, \n","#     remove_columns=train_squad_v2.column_names\n","# )\n","# test_squad_v2 = test_squad_v2.map(\n","#     preprocess_batch_llama, \n","#     batched=True, \n","#     remove_columns=test_squad_v2.column_names\n","# )\n","\n","testing_2 = testing_copy_2.map(\n","    preprocess_batch_llama, \n","    batched=True, \n","    remove_columns=testing_copy_2.column_names\n",")\n","\n","# train_squad_v2 = train_squad_v2.map(preprocess_batch, batched=True, remove_columns=train_squad_v2.column_names)\n","# test_squad_v2 = test_squad_v2.map(preprocess_batch, batched=True, remove_columns=test_squad_v2.column_names)\n","# print(\"done\")\n","# print(testing_2.column_names)\n","print(len(train_squad_v2)/len(test_squad_v2))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:38:56.214036Z","iopub.status.busy":"2024-11-22T23:38:56.213695Z","iopub.status.idle":"2024-11-22T23:38:56.220003Z","shell.execute_reply":"2024-11-22T23:38:56.219130Z","shell.execute_reply.started":"2024-11-22T23:38:56.214007Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['id', 'title', 'context', 'question', 'answers']\n"]}],"source":["# !pip install rouge_score\n","# train_squad_v2.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"])\n","# test_squad_v2.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"])\n","\n","testing_2.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"])\n","\n","\n","print(train_squad_v2.column_names)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:38:59.281780Z","iopub.status.busy":"2024-11-22T23:38:59.281411Z","iopub.status.idle":"2024-11-22T23:38:59.286219Z","shell.execute_reply":"2024-11-22T23:38:59.285420Z","shell.execute_reply.started":"2024-11-22T23:38:59.281748Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n"]}],"source":["print(testing_2.column_names)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T21:58:07.134885Z","iopub.status.busy":"2024-11-22T21:58:07.134562Z","iopub.status.idle":"2024-11-22T21:58:08.280157Z","shell.execute_reply":"2024-11-22T21:58:08.279047Z","shell.execute_reply.started":"2024-11-22T21:58:07.134846Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["# !pip install rouge_score\n","# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n","!export TOKENIZERS_PARALLELISM=true\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T23:39:12.197340Z","iopub.status.busy":"2024-11-22T23:39:12.196997Z","iopub.status.idle":"2024-11-22T23:39:12.204090Z","shell.execute_reply":"2024-11-22T23:39:12.202404Z","shell.execute_reply.started":"2024-11-22T23:39:12.197303Z"},"trusted":true},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","       \n","        input_ids = inputs.get(\"input_ids\")\n","        attention_mask = inputs.get(\"attention_mask\")\n","        start_positions = inputs.get(\"start_positions\")\n","        end_positions = inputs.get(\"end_positions\")\n","\n","        \n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, \n","                        start_positions=start_positions, end_positions=end_positions)\n","\n","       \n","        loss = outputs.loss\n","\n","       \n","        if return_outputs:\n","            return loss, outputs\n","        return loss"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T21:58:08.291020Z","iopub.status.busy":"2024-11-22T21:58:08.290778Z","iopub.status.idle":"2024-11-22T21:58:10.032129Z","shell.execute_reply":"2024-11-22T21:58:10.031306Z","shell.execute_reply.started":"2024-11-22T21:58:08.290997Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4943283200\n","4957667328\n","4943283200\n"]}],"source":["torch.cuda.empty_cache()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","# model = torch.nn.DataParallel(model, device_ids=[0, 1])\n","print(torch.cuda.memory_allocated())\n","print(torch.cuda.memory_reserved())\n","print(torch.cuda.max_memory_allocated())"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-11-22T22:18:50.305074Z","iopub.status.busy":"2024-11-22T22:18:50.304707Z","iopub.status.idle":"2024-11-22T22:19:15.846935Z","shell.execute_reply":"2024-11-22T22:19:15.845415Z","shell.execute_reply.started":"2024-11-22T22:18:50.305042Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/6 00:02 < 00:06, 0.45 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>6.474200</td>\n","      <td>No log</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"ename":"KeyError","evalue":"\"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3022\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n","\u001b[0;31mKeyError\u001b[0m: 'eval_f1'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 118\u001b[0m\n\u001b[1;32m    108\u001b[0m trainer_squad_v2 \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    109\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    110\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args_squad_v2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Train the model (resume from checkpoint if available)\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtrainer_squad_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Save the final model after training\u001b[39;00m\n\u001b[1;32m    121\u001b[0m trainer_squad_v2\u001b[38;5;241m.\u001b[39msave_model(training_args_squad_v2\u001b[38;5;241m.\u001b[39moutput_dir)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2487\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2491\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2918\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2918\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3024\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3022\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m metrics[metric_to_check]\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m-> 3024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   3025\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, which is not found in the evaluation metrics. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3026\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3027\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m   3029\u001b[0m operator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgreater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgreater_is_better \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mless\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m operator(metric_value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric)\n\u001b[1;32m   3034\u001b[0m ):\n","\u001b[0;31mKeyError\u001b[0m: \"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\""]}],"source":["import evaluate\n","import os\n","import numpy as np\n","from transformers import Trainer, TrainingArguments, AutoModelForQuestionAnswering, AdamW\n","import torch\n","import torch.utils.checkpoint as checkpoint\n","\n","torch.cuda.empty_cache()\n","\n","\n","# model.eval()  \n","# torch.no_grad() \n","\n","training_args_squad_v2 = TrainingArguments(\n","    logging_first_step=True,  \n","    output_dir=\"./llama_squad_v2_model_test\",\n","    eval_strategy=\"epoch\",  \n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=3,\n","    seed=1, \n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\", \n","    save_total_limit=3,  \n","    overwrite_output_dir=True, \n","    logging_dir=\"./logs\", \n","    logging_steps=10, \n","    disable_tqdm=False,  \n","    report_to=\"tensorboard\",\n","    fp16=True,  \n","    gradient_accumulation_steps=2,\n","    # eval_accumulation_steps =1,\n","    dataloader_num_workers=4,\n","    remove_unused_columns=False,\n","    no_cuda=False,\n","    dataloader_pin_memory=True,\n",")\n","\n","\n","squad_v2_metric = evaluate.load(\"squad_v2\")\n","# rouge_metric = evaluate.load(\"rouge\")\n","# bleu_metric = evaluate.load(\"bleu\")\n","# meteor_metric = evaluate.load(\"meteor\")\n","\n","\n","def compute_metrics_2(eval_pred):\n","    print(\"Inside compute_metrics!\")\n","    logits, labels = eval_pred\n","    \n","   \n","    predictions = np.argmax(logits, axis=-1)\n","    print(predictions)\n","    print(labels)\n","   \n","    formatted_preds = [\n","        {\"prediction_text\": str(pred), \"id\": str(i), 'no_answer_probability': 0.} \n","        for i, pred in enumerate(predictions)\n","    ]\n","    \n","    formatted_labels = [\n","        {\n","            \"answers\": {\"text\": [str(label)], \"answer_start\": [0]},\n","            \"id\": str(i)\n","        } \n","        for i, label in enumerate(labels)\n","    ]\n","    \n","    print(formatted_preds)\n","    print(formatted_labels)\n","    return squad_v2_metric.compute(\n","        predictions=formatted_preds, \n","        references=formatted_labels\n","    )\n","\n","\n","# def compute_metrics(eval_pred): \n","#     print(\"Inside compute_metrics!\")\n","#     logits, labels = eval_pred\n","    \n","#     predictions = np.argmax(logits, axis=-1)\n","    \n","\n","#     predicted_texts = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","#     true_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","#     formatted_preds = [\n","#         {\"prediction_text\": pred, \"id\": str(i), 'no_answer_probability': 0.0} \n","#         for i, pred in enumerate(predicted_texts)\n","#     ]\n","    \n","#     formatted_labels = [\n","#         {\"answers\": {\"text\": [true], \"answer_start\": [0]}, \"id\": str(i)} \n","#         for i, true in enumerate(true_texts)\n","#     ]\n","    \n","#     results = squad_v2_metric.compute(predictions=formatted_preds, references=formatted_labels)\n","#     print(results)\n","#     return results \n","\n","\n","trainer_squad_v2 = CustomTrainer(\n","    model=model,\n","    args=training_args_squad_v2,\n","    train_dataset=testing_2,\n","    eval_dataset=testing_2,\n","    compute_metrics=compute_metrics_2,\n","    tokenizer=tokenizer,\n",")\n","\n","\n","trainer_squad_v2.train()\n","\n","\n","trainer_squad_v2.save_model(training_args_squad_v2.output_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-22T21:58:36.875625Z","iopub.status.idle":"2024-11-22T21:58:36.875963Z","shell.execute_reply":"2024-11-22T21:58:36.875830Z","shell.execute_reply.started":"2024-11-22T21:58:36.875813Z"},"trusted":true},"outputs":[],"source":["from evaluate import load\n","squad_v2_metric = load(\"squad_v2\")\n","predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22', 'no_answer_probability': 0.0}]\n","references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n","results = squad_v2_metric.compute(predictions=predictions, references=references)\n","print(results)\n","\n","#From https://huggingface.co/spaces/evaluate-metric/squad_v2"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
